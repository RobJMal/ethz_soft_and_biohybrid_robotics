{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Giyex3Yle1id"
      },
      "source": [
        "# Training a Lunar Lander to Safely Land with RL (30 points)\n",
        "\n",
        "In this assignment, we will use RL to train a model to control a \"Lunar Lander\" in the OpenAI gym Environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJVsvJln2lks"
      },
      "source": [
        "Let's get started!\n",
        "\n",
        "First, make sure that pytorch is installed in your python environment, using the guide in here https://pytorch.org/get-started/locally/. Instructions will vary depending on whether you are running torch from CPU or GPU. We recommend running this exercise on GPU if possible. After installing pytorch, restart the kernel of the jupyter notebook (\"refresh\" button above).\n",
        "\n",
        "Then, we start by installing the necessary additonal packages, such as Gym."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6SDJZh8zMuR",
        "outputId": "9c5b3ece-6a21-4768-b5ee-0c010fa41d7d"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start as '_psutil_linux' could not be imported from 'most likely due to a circular import'.\n",
            "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install gym[box2d] pygame matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzFpM-0tzMuT"
      },
      "source": [
        "Let's then import all necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTBB78Fie1ij"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "# For visualization\n",
        "from IPython.display import clear_output, display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJYKdOa2e1im"
      },
      "source": [
        "Now let's create the LunarLander-v2 environment and see how to operate on it. An environment has the following properties:\n",
        "\n",
        "* **Action space** is the set of possible actions that we can perform at each step of the simulation\n",
        "* **Observation space** is the space of observations that we can make\n",
        "\n",
        "See https://www.gymlibrary.dev/environments/box2d/lunar_lander/ for a more complete description of the Lunar Lander environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RryaKN0Ce1in",
        "outputId": "75a0cd8d-2909-40f0-94c5-ff6be6e39a5d"
      },
      "outputs": [],
      "source": [
        "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n",
        "\n",
        "print(f\"Action space: {env.action_space}\")\n",
        "print(f\"Observation space: {env.observation_space}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSs_exGDe1ip"
      },
      "source": [
        "Let's see how the simulation works. The following loop runs the simulation, until `env.step` returns `terminated` or `truncated`. We are now choosing random actions using `env.action_space.sample()`, which means the experiment will probably fail.\n",
        "\n",
        "The total reward is attained by summing rewards at every step, according to the following rules:\n",
        "\n",
        "> Reward for moving from the top of the screen to the landing pad and coming to rest is about 100-140 points. If the lander moves away from the landing pad, it loses reward. If the lander crashes, it receives an additional -100 points. If it comes to rest, it receives an additional +100 points. Each leg with ground contact is +10 points. Firing the main engine is -0.3 points each frame. Firing the side engine is -0.03 points each frame. Solved is 200 points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nbDpKOMe1iq",
        "outputId": "22c3df59-b4e7-4398-c401-8e59ae3445a7"
      },
      "outputs": [],
      "source": [
        "state = env.reset()\n",
        "total_reward = 0\n",
        "render = False\n",
        "if render:\n",
        "    img = plt.imshow(env.render())\n",
        "\n",
        "while True:\n",
        "    if render:\n",
        "        img.set_data(env.render())\n",
        "        plt.axis('off')\n",
        "        display(plt.gcf())\n",
        "        clear_output(wait=True)\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, terminated, truncated, info = env.step(action)\n",
        "    total_reward += reward\n",
        "    if terminated or truncated:\n",
        "        break\n",
        "print(f\"Total reward: {total_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQdLfQx8Cwxd"
      },
      "source": [
        "After a few seconds, you should see an output with the total reward achieved in this episode. Congratulations, your env is up and running!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-uWbKk3Cwxe"
      },
      "source": [
        "### Task 2a (4 points)\n",
        "Although total reward can quantify how good or bad the episode goes, it is hard to imagine what actually happens within the env. Therefore, it is important for us to visualize the episode.\n",
        "\n",
        "Copy the code from the previous block into the following block, set `render` to `True` and run the code. Now you should be able to see a purple moon lander landing (or much more likely, crashing).\n",
        "\n",
        "<font color='red'>**Please make a screen recording of the visualization and report on the final Total Reward output.**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "7g91pQjnCwxe",
        "outputId": "ddf2f308-c054-48f9-db86-5f6692541e77"
      },
      "outputs": [],
      "source": [
        "#####\n",
        "# COMPLETE CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "#####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_g2UmhCzMuV"
      },
      "source": [
        "The total reward is likely negative because the action is randomly sampled from the action space. During reinforcement learning, our goal is to train a **policy** $\\pi$, that for each state $s$ will tell us which action $a$ to take, so essentially $a = \\pi(s)$.\n",
        "\n",
        "If you want probabilistic solution, you can think of policy as returning a set of probabilities for each action, i.e. $\\pi(a|s)$ would mean a probability that we should take action $a$ at state $s$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSeElbF165q2"
      },
      "source": [
        "# Deep Q-Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBry3VrtjY_r"
      },
      "source": [
        "### Task 2b (8 points)\n",
        "\n",
        "As seen in the class, a popular Reinforcement Learning algorithm is Q-learning, in which the \"Q function\" is approximated and used to define a policy. Specifically, we use a neural network to represent the Q function.\n",
        "\n",
        "Since LunarLander-v2 environment is a simple environment, we don't need a complicated architecture. A simple feed-forward neural network will suffice.\n",
        "\n",
        "<font color='red'>**Please complete the code to make a simple 3-layer network with hidden layer input and output size 64.**</font> You need to build the model and also write the corresponding forward function.\n",
        "\n",
        "Hint: the architecture should be three fully connected layers with ReLU in between."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfziXfMnjlue"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    \"\"\"Actor (Policy) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "        \"\"\"\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        #####\n",
        "        # COMPLETE CODE HERE\n",
        "        \n",
        "        \n",
        "\n",
        "        #####\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
        "        #####\n",
        "        # COMPLETE CODE HERE\n",
        "        \n",
        "        \n",
        "        return None\n",
        "        #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF6TGEbCjyru"
      },
      "source": [
        "Deep Q Learning uses a buffer to store experiments and learn from them. It also uses discounted rewards based on a gamma parameter.\n",
        "\n",
        "We then specify these parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soYZ6N9pj174"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "BATCH_SIZE = 64         # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 1e-3              # for soft update of target parameters\n",
        "LR = 5e-4               # learning rate\n",
        "UPDATE_EVERY = 4        # how often to update the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxzlz-irj7Ib"
      },
      "source": [
        "### Task 2c (8 points)\n",
        "\n",
        "Let's now define the \"Agent\", which uses the Q network to choose how to act in the environment.\n",
        "\n",
        "With \"act\", it performs an action given the current state and its current Q function approximation. With \"step\", it observes the response of the environment, adds it to its buffer, and performs a learning step (\"learn\"), once every UPDATE_EVERY.\n",
        "\n",
        "In the learning step, random minibatch is first sampled from the memory. The target value can be computed from Bellman equation.\n",
        "\n",
        "$$\n",
        "  Q(s_t, a_t) = r_t + \\gamma(1-d)\\max_aQ(s_{t+1}, a_{t+1})\n",
        "$$\n",
        "\n",
        ", where $\\gamma$ is the discount factor, $d$ equals 1 if the current state is the last state of the episode, and $\\max_aQ(s_{t+1}, a_{t+1})$ is the next maximum estimated value extracted from the target network.\n",
        "\n",
        "If we want to maximize Q, then we need to make sure that the expected Q value equals the optimal target Q value. So to train an RL algorithm we need to minimize a mean squared error between the two.\n",
        "\n",
        "<font color='red'>**Please complete the code</font> to calculate\n",
        "1. the target value\n",
        "\n",
        "1. the MSE loss between the target Q value and the expected Q value from local network.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tV8KUz3Wj9Uf"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Agent():\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        # Q-Network\n",
        "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
        "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        # Save experience in replay memory\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # Learn every UPDATE_EVERY time steps.\n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "        if self.t_step == 0:\n",
        "            # If enough samples are available in memory, get random subset and learn\n",
        "            if len(self.memory) > BATCH_SIZE:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences, GAMMA)\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        \"\"\"Returns actions for given state as per current policy.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state (array_like): current state\n",
        "            eps (float): epsilon, for epsilon-greedy action selection\n",
        "        \"\"\"\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        # Epsilon-greedy action selection\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        \"\"\"Update value parameters using given batch of experience tuples.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "        # Obtain random minibatch of tuples from D\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # Extract next maximum estimated value from target network\n",
        "        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "        #####\n",
        "        # COMPLETE CODE HERE\n",
        "        ### Calculate target value from bellman equation\n",
        "        q_targets = None\n",
        "\n",
        "        #####\n",
        "\n",
        "        # Calculate expected value from local network\n",
        "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        #####\n",
        "        # COMPLETE CODE HERE\n",
        "        ### Loss calculation (use Mean squared error)\n",
        "        loss = None\n",
        "\n",
        "        #####\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # ------------------- update target network ------------------- #\n",
        "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            local_model (PyTorch model): weights will be copied from\n",
        "            target_model (PyTorch model): weights will be copied to\n",
        "            tau (float): interpolation parameter\n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i79FEDSkX67"
      },
      "source": [
        "We define here the Replay Buffer used for storing experiences and sampling them for learning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf94lxD4kZCQ"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.buffer_size = buffer_size\n",
        "        self.memory_states = torch.zeros(self.buffer_size, 8).to(device)\n",
        "        self.memory_actions = torch.zeros(self.buffer_size, 1).long().to(device)\n",
        "        self.memory_rewards = torch.zeros(self.buffer_size, 1).to(device)\n",
        "        self.memory_next_states = torch.zeros(self.buffer_size, 8).to(device)\n",
        "        self.memory_dones = torch.zeros(self.buffer_size, 1).int().to(device)\n",
        "        self.batch_size = batch_size\n",
        "        self.count = 0\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        memory_idx = self.count % self.buffer_size\n",
        "\n",
        "        self.memory_states[memory_idx] = torch.from_numpy(state).float()\n",
        "        self.memory_actions[memory_idx] = torch.Tensor([action]).long()\n",
        "        self.memory_rewards[memory_idx] = float(reward)\n",
        "        self.memory_next_states[memory_idx] = torch.from_numpy(next_state).float()\n",
        "        self.memory_dones[memory_idx] = int(done)\n",
        "\n",
        "        self.count += 1\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "\n",
        "        random_idxs = np.array(random.sample(range(len(self)), k=self.batch_size))\n",
        "\n",
        "        states = self.memory_states[random_idxs]\n",
        "        actions = self.memory_actions[random_idxs]\n",
        "        rewards = self.memory_rewards[random_idxs]\n",
        "        next_states = self.memory_next_states[random_idxs]\n",
        "        dones = self.memory_dones[random_idxs]\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return min(self.count, self.buffer_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "414JS3S7ke3I"
      },
      "source": [
        "And here is the function used to run the overall training process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2s4M0EMfkiTx"
      },
      "outputs": [],
      "source": [
        "def dqn(agent, n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "    \"\"\"Deep Q-Learning.\n",
        "\n",
        "    Params\n",
        "    ======\n",
        "        agent (Agent): RL agent\n",
        "        n_episodes (int): maximum number of training episodes\n",
        "        max_t (int): maximum number of timesteps per episode\n",
        "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
        "        eps_end (float): minimum value of epsilon\n",
        "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
        "    \"\"\"\n",
        "    self.agent = agent\n",
        "    start_time = time.time()\n",
        "    scores = []                        # list containing scores from each episode\n",
        "    scores_window = deque(maxlen=100)  # last 100 scores\n",
        "    eps = eps_start                    # initialize epsilon\n",
        "\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()[0]\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "        scores_window.append(score)       # save most recent score\n",
        "        scores.append(score)              # save most recent score\n",
        "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
        "        elapsed_time_s = time.time() - start_time\n",
        "        end_char = \"\\n\" if i_episode % 100 == 0 else \"\"\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tCount: {:.2f}\\tElapsed time: {:.2f} sec'.format(i_episode, np.mean(scores_window), len(agent.memory), elapsed_time_s), end=end_char)\n",
        "\n",
        "    print('\\nEnvironment trained for {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "    torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-R9MY_X17QT"
      },
      "source": [
        "### Task 2d (10 points)\n",
        "\n",
        "Let's train our agent over 1000 episodes (this will take a while):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BsjYi5T19Gw",
        "outputId": "2cb47322-a51d-4ff2-df42-1143aee9e546"
      },
      "outputs": [],
      "source": [
        "agent = Agent(state_size=8, action_size=4, seed=0)\n",
        "scores = dqn(agent, n_episodes=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOsQJC_bk3PT"
      },
      "source": [
        "<font color='red'>**Please report on the final average score**</font>\n",
        "\n",
        "You need to achieve <font color='red'>a final average score of at least 200 with a safe landing animation</font> to get full points for this task. You can tune `n_episodes` to train longer or shorter.\n",
        "\n",
        "You can plot the learning process with the following code. If your reward is not improving, go back and carefully check your previous code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUI-JwPZk898",
        "outputId": "cf697338-e610-41a6-c54e-aff0456397f2"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(len(scores)), scores)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1SW0_QWlKO8"
      },
      "source": [
        "<font color='red'>**Please save the reward history plot and add it to your report.**</font>\n",
        "\n",
        "Now let's render the result, "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghJdaFA2lOOa",
        "outputId": "8228615f-3f86-4782-e042-ccb433a9c364"
      },
      "outputs": [],
      "source": [
        "agent = Agent(state_size=8, action_size=4, seed=0)\n",
        "state = env.reset()[0]\n",
        "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
        "total_reward = 0\n",
        "render = True\n",
        "if render:\n",
        "    img = plt.imshow(env.render())\n",
        "\n",
        "while True:\n",
        "    if render:\n",
        "        img.set_data(env.render())\n",
        "        plt.axis('off')\n",
        "        display(plt.gcf())\n",
        "        clear_output(wait=True)\n",
        "    action = env.action_space.sample()\n",
        "    action = agent.act(state)\n",
        "    state, reward, terminated, truncated, info = env.step(action)\n",
        "    total_reward += reward\n",
        "    if terminated or truncated:\n",
        "        break\n",
        "print(f\"Total reward: {total_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPbtPxfbe1iy"
      },
      "source": [
        "Congratulations, you just solved the Lunar Lander task! \n",
        "\n",
        "<font color='red'>**Please make a screen recording of the episode animation and report on the total reward.**</font>\n",
        "\n",
        "You can play around with the parameter to get a feeling for how important these are for solving the task. You can also try to implement other RL algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font color='red'>**Please make sure to save ALL cell outputs in Jupyter Notebook and add the file to your code archive.**</font> Do NOT clear cell outputs. Submission without cell outputs will be penalized up to half the points of the task."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "16af2a8bbb083ea23e5e41c7f5787656b2ce26968575d8763f2c4b17f9cd711f"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
